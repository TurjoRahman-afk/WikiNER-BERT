# NER Model Configuration

# Model settings
model:
  name: "bert-base-uncased"  # Options: bert-base-uncased, roberta-base, distilbert-base-uncased
  max_length: 128
  num_labels: null  # Will be set automatically based on dataset

# Training settings
training:
  output_dir: "./models/checkpoints"
  learning_rate: 2.0e-5
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 16
  num_train_epochs: 3
  weight_decay: 0.01
  warmup_steps: 1000   # more warmup steps for large datasets 
  logging_steps: 500   # Log less frequently 
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: true
  save_total_limit: 2
  # Early stopping: stop training if validation metric doesn't improve
  early_stopping_patience: 2
  seed: 42

  # Additional training settings
  logging_dir: "./logs"
  metric_for_best_model: "f1"
  greater_is_better: true

# Data settings
data:
  dataset_name: "wikiner"  # Modern version without loading script
  processed_data_dir: "./data/processed"
  raw_data_dir: "./data/raw"
  cache_dir: "./data/cache"

# Paths
paths:
  model_dir: "./models"
  model_save_dir: "./models/best_model"
  results_dir: "./results"
  logs_dir: "./logs"

# Inference settings
inference:
  batch_size: 32
  max_length: 128
  logging_dir: "./logs"
